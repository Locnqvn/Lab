{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Lx5yrRT7kD6xnJzDONCRgO1FZpUTEeOb","authorship_tag":"ABX9TyMWKnKKbYXMow5tZF9nrE8M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CtEfeyGsodyz","colab_type":"code","colab":{}},"source":["import tensorflow as tf \n","import numpy as np \n","import random\n","\n","\n","class MLP:\n","    def __init__(self,size_hidden,dim_feature,n_class):\n","        self.dim_feature=dim_feature\n","        self.size_hidden=size_hidden\n","        self.n_class=n_class\n","        self.X=None\n","        self.Y_true=None\n","    \n","    def build_graph(self):\n","        tf.compat.v1.disable_eager_execution()\n","        self.X=tf.compat.v1.placeholder(tf.float32,shape=[None,self.dim_feature])\n","        self.Y_true=tf.compat.v1.placeholder(tf.int32,shape=[None,])\n","\n","        weight1= tf.Variable(name=\"weight_input_hidden\",\n","                            initial_value=tf.random.normal([self.dim_feature,self.size_hidden]))\n","        bias1= tf.Variable(name=\"bias_hidden\",\n","                            initial_value=tf.random.normal([self.size_hidden]))\n","\n","        weight2=tf.Variable(name=\"weight_hidden_output\",\n","                            initial_value=tf.random.normal([self.size_hidden,self.n_class]))\n","        bias2=tf.Variable(name=\"bias_ouput\",\n","                            initial_value=tf.random.normal([self.n_class]))\n","\n","        hidden=tf.matmul(self.X,weight1)+bias1\n","        hidden=tf.sigmoid(hidden)\n","        logits=tf.matmul(hidden,weight2)+bias2\n","\n","        labels_one_hot= tf.one_hot(indices=self.Y_true,depth=self.n_class,dtype=tf.int32)\n","        loss=tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,logits=logits)\n","        loss= tf.reduce_mean(loss)\n","        prob= tf.nn.softmax(logits)\n","        pre_labels=tf.argmax(prob,axis=1)\n","        \n","        return pre_labels,loss\n","\n","    def trainer(self,lr,loss):\n","        train= tf.compat.v1.train.AdamOptimizer(lr).minimize(loss)\n","        return train\n","\n","\n","\n","class DataReader:\n","    def __init__(self,path,size_batch,dim_feature):\n","        def convert_to_vector(doc,dim_feature):\n","            vector=np.zeros(dim_feature)\n","            feas=doc.split()\n","            for fea in feas:\n","                v=fea.split(':')\n","                vector[int(v[0])]=float(v[1])\n","            return vector\n","\n","        with open(path,'r') as f:\n","            d_line=f.read().splitlines()\n","        self.x=[]\n","        self.y=[]\n","        self.size_batch=size_batch\n","        for d in d_line:\n","            fea=d.split('<<>>')\n","            self.y.append(fea[0])\n","            fea_vector=convert_to_vector(fea[-1],dim_feature)\n","            self.x.append(fea_vector)\n","        self.n_doc=len(d_line)\n","        self.n_batch=int(np.ceil(self.n_doc/size_batch))\n","        rd=random.sample(range(self.n_doc),self.n_doc)\n","        self.x=np.array(self.x)[rd]\n","        self.y=np.array(self.y)[rd]\n","        self.batch_id=0\n","        self.epoch=0\n","    \n","    def next_batch(self):\n","        if(self.batch_id>=self.n_batch):\n","            self.batch_id=0\n","            self.epoch+=1\n","            rd=random.sample(range(self.n_doc),self.n_doc)\n","            self.x=self.x[rd]\n","            self.y=self.y[rd]\n","        \n","        start=self.batch_id*self.size_batch\n","        end=start+self.size_batch\n","        self.batch_id+=1\n","        batch_x=self.x[start:end]\n","        batch_y=self.y[start:end]\n","\n","        return batch_x,batch_y\n","\n","def save_paras(name,value,path):\n","    value_form=[]\n","    if(len(value.shape)==1):\n","        value_form=[str(number) for number in value]\n","    else:\n","        for row in range(len(value)):\n","            value_form.append(' '.join([str(number) for number in value[row]]))\n","    with open(path+'/'+name+'.txt','w') as f:\n","        f.write('\\n'.join(value_form))\n","\n","def restore_paras(path_in,name):\n","    with open(path_in+'/'+name+'.txt','r') as f:\n","        data=f.read().splitlines()\n","    values=[]\n","    if(len(data)==1):\n","        value=[float(number) for number in data.split()]\n","    else:\n","        for d in data:\n","            values.append([float(number) for number in d.split()])\n","    \n","    return values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vlqurv6yec1W","colab_type":"code","outputId":"41dbf0ec-2820-4302-eece-dcbf5cecec17","executionInfo":{"status":"ok","timestamp":1592220318116,"user_tz":-420,"elapsed":324447,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if __name__ == \"__main__\":\n","    '''\n","    with open(\"\",'r') as f:\n","        dim_feature=len(f.read().splitlines())\n","    '''\n","    path_train=\"/content/drive/My Drive/Project2/Datasets/train_tfidf_df=3\"\n","    path_test=\"/content/drive/My Drive/Project2/Datasets/test_tfidf\"\n","    dim_feature=20167\n","    mlp=MLP(50,dim_feature,20)\n","    pre_label,loss=mlp.build_graph()\n","    trainer=mlp.trainer(0.01,loss=loss)\n","\n","    init = tf.compat.v1.global_variables_initializer()\n","    with tf.compat.v1.Session() as sess:\n","        data_train=DataReader(path_train,50,20167)\n","        data_test=DataReader(path_test,50,20167)\n","        it,MAX_epoch=0,100\n","        sess.run(init)\n","        while it<MAX_epoch:\n","            batch_train_x,batch_train_y=data_train.next_batch()\n","            pre_val,loss_val,_= sess.run(\n","                [pre_label,loss,trainer],\n","                feed_dict={\n","                    mlp.X:batch_train_x,\n","                    mlp.Y_true:batch_train_y\n","                }\n","            )\n","            if(it<data_train.epoch):\n","                print(\"epoch {}, =loss ={}\".format(data_train.epoch,loss_val))\n","            it=data_train.epoch\n","        \n","        trainable_variables=tf.compat.v1.trainable_variables()\n","        for variable in trainable_variables:\n","            save_paras(\n","                name=variable.name,\n","                value=variable.eval(),\n","                path='/content/drive/My Drive/Lab/paras'\n","            )\n","        "],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","epoch 1, =loss =0.5492036938667297\n","epoch 2, =loss =0.20984907448291779\n","epoch 3, =loss =0.03416093811392784\n","epoch 4, =loss =0.009996453300118446\n","epoch 5, =loss =0.007882528007030487\n","epoch 6, =loss =0.006103637162595987\n","epoch 7, =loss =0.009011822752654552\n","epoch 8, =loss =0.008451980538666248\n","epoch 9, =loss =0.0032189900521188974\n","epoch 10, =loss =0.010175035335123539\n","epoch 11, =loss =0.0022902474738657475\n","epoch 12, =loss =0.0016660152468830347\n","epoch 13, =loss =0.001060745446011424\n","epoch 14, =loss =0.0007377241272479296\n","epoch 15, =loss =0.0010383084882050753\n","epoch 16, =loss =0.0011014753254130483\n","epoch 17, =loss =0.000725111342035234\n","epoch 18, =loss =0.0007288033375516534\n","epoch 19, =loss =0.000783333380240947\n","epoch 20, =loss =0.0009273638715967536\n","epoch 21, =loss =0.000646027794573456\n","epoch 22, =loss =0.0003212660667486489\n","epoch 23, =loss =0.0002680794568732381\n","epoch 24, =loss =0.0003647305420599878\n","epoch 25, =loss =0.00041475656325928867\n","epoch 26, =loss =0.0002408410655334592\n","epoch 27, =loss =0.00017399556236341596\n","epoch 28, =loss =0.00021762773394584656\n","epoch 29, =loss =0.00016261421842500567\n","epoch 30, =loss =0.0001044094969984144\n","epoch 31, =loss =0.0001979885419132188\n","epoch 32, =loss =8.963794243754819e-05\n","epoch 33, =loss =5.6193537602666765e-05\n","epoch 34, =loss =8.153613453032449e-05\n","epoch 35, =loss =0.00010715534153860062\n","epoch 36, =loss =7.179605745477602e-05\n","epoch 37, =loss =6.290203600656241e-05\n","epoch 38, =loss =4.920117135043256e-05\n","epoch 39, =loss =3.5453402233542874e-05\n","epoch 40, =loss =3.685239425976761e-05\n","epoch 41, =loss =3.349332109792158e-05\n","epoch 42, =loss =1.7365609892294742e-05\n","epoch 43, =loss =4.800861279363744e-05\n","epoch 44, =loss =0.00010757583368103951\n","epoch 45, =loss =6.527946970891207e-05\n","epoch 46, =loss =4.3621370423352346e-05\n","epoch 47, =loss =3.600245327106677e-05\n","epoch 48, =loss =1.1951636224694084e-05\n","epoch 49, =loss =1.7325219232589006e-05\n","epoch 50, =loss =2.8927854145877063e-05\n","epoch 51, =loss =2.4408991521340795e-05\n","epoch 52, =loss =1.5062740203575231e-05\n","epoch 53, =loss =1.8915277905762196e-05\n","epoch 54, =loss =7.27882206774666e-06\n","epoch 55, =loss =7.517248377553187e-06\n","epoch 56, =loss =3.712151055879076e-06\n","epoch 57, =loss =3.2591692615824286e-06\n","epoch 58, =loss =0.00028873165138065815\n","epoch 59, =loss =2.0790016606042627e-06\n","epoch 60, =loss =2.9444368010445032e-06\n","epoch 61, =loss =2.4366299840039574e-06\n","epoch 62, =loss =8.76809917826904e-06\n","epoch 63, =loss =2.562977670095279e-06\n","epoch 64, =loss =3.7526456253544893e-06\n","epoch 65, =loss =4.277160314813955e-06\n","epoch 66, =loss =0.0006393287912942469\n","epoch 67, =loss =0.011140809394419193\n","epoch 68, =loss =1.425739128535497e-06\n","epoch 69, =loss =8.010847523109987e-07\n","epoch 70, =loss =1.735680598358158e-06\n","epoch 71, =loss =7.057179232106137e-07\n","epoch 72, =loss =1.4567334574167035e-06\n","epoch 73, =loss =0.005416551139205694\n","epoch 74, =loss =5.841247343596478e-07\n","epoch 75, =loss =5.316726401360938e-07\n","epoch 76, =loss =1.25407882478612e-06\n","epoch 77, =loss =8.773744752943458e-07\n","epoch 78, =loss =0.011664383113384247\n","epoch 79, =loss =3.004072368639754e-07\n","epoch 80, =loss =4.1007959339367517e-07\n","epoch 81, =loss =4.816044452127244e-07\n","epoch 82, =loss =5.626666279567871e-07\n","epoch 83, =loss =5.578957029683806e-07\n","epoch 84, =loss =1.52587830370976e-07\n","epoch 85, =loss =6.389590225808206e-07\n","epoch 86, =loss =3.4809016824510763e-07\n","epoch 87, =loss =3.0279130669441656e-07\n","epoch 88, =loss =2.0742403705753532e-07\n","epoch 89, =loss =2.503391556274437e-07\n","epoch 90, =loss =1.311301929263209e-07\n","epoch 91, =loss =1.049041600253986e-07\n","epoch 92, =loss =1.0251996940269237e-07\n","epoch 93, =loss =6.198882118724214e-08\n","epoch 94, =loss =6.437300470452101e-08\n","epoch 95, =loss =7.152556236178498e-08\n","epoch 96, =loss =4.291533528544278e-08\n","epoch 97, =loss =1.5974039513366733e-07\n","epoch 98, =loss =5.483626352997817e-08\n","epoch 99, =loss =3.814696825088504e-08\n","epoch 100, =loss =7.629392939634272e-08\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YQCFFp8Fk-XW","colab_type":"code","outputId":"8031bee7-636a-493d-fbf6-b3c7397b523f","executionInfo":{"status":"ok","timestamp":1592222495914,"user_tz":-420,"elapsed":6679,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with tf.compat.v1.Session() as sess:\n","    data_test=DataReader(path_test,50,20167)\n","    sess.run(init)\n","    for variable in trainable_variables:\n","        saved_value=restore_paras('/content/drive/My Drive/Lab/paras',variable.name)\n","        saved_value=np.squeeze(saved_value)\n","        assign_op=variable.assign(saved_value)\n","        sess.run(assign_op)\n","    num_true_pre=0\n","    while True:\n","        batch_test_x,batch_test_y=data_test.next_batch()\n","        pre_val= sess.run(\n","            pre_label,\n","            feed_dict={\n","                mlp.X:batch_test_x,\n","                mlp.Y_true:batch_test_y              \n","            }\n","        )\n","        pre_val=np.array(pre_val,dtype=int)\n","        batch_test_y=np.array(batch_test_y,dtype=int)\n","        num_true_pre+=sum(pre_val==batch_test_y)\n","        if(data_test.epoch==1):\n","            break\n","    print(float(num_true_pre/len(data_test.y)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.7619490175252257\n"],"name":"stdout"}]}]}