{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"RNN_colab_ver.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Hcms8G5CJwei","colab_type":"code","outputId":"a9c0e4cd-8814-46d7-ad82-42e411ea49da","executionInfo":{"status":"ok","timestamp":1592224525309,"user_tz":-420,"elapsed":2126,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVfeaceH6EKW","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","import numpy as np \n","import os\n","import re\n","\n","class Tranf_encode:\n","    def __init__(self,max_doc_length,min_df):\n","        self._vocab=[]\n","        self._max_doc_length=max_doc_length\n","        self._min_df=min_df\n","    \n","    def isappear(self,word,wordslist):\n","        l=0\n","        r=len(wordslist)-1\n","        while(l<=r):\n","            m=int((r+l)/2)\n","            if(word==wordslist[m]):\n","                return m\n","            else:\n","                if(word<wordslist[m]):\n","                    r=m-1\n","                else:\n","                    l=m+1\n","        return -1\n","\n","    def fit(self,path_train):\n","        def get_data_and_vocab(path_train):\n","            data=[]\n","            vocab_count=defaultdict(int)\n","            list_folder=os.listdir(path_train)\n","            list_folder.sort()\n","            i_folder=0\n","            for folder in list_folder:\n","                path_folder=path_train+'/'+folder\n","                list_doc=os.listdir(path_folder)\n","                list_doc.sort()\n","                for doc in list_doc:\n","                    path_file=path_folder+'/'+doc\n","                    with open(path_file,'r',errors='ignore') as f:\n","                        text=f.read()\n","                    words=re.split('\\W+',text)\n","                    for word in words:\n","                        vocab_count[word]+=1\n","                    content=' '.join(words)\n","                    ifo_doc=str(i_folder)+'<fff>'+str(doc)+'<fff>'+content\n","                    data.append(ifo_doc)\n","                i_folder+=1\n","            \n","            self._vocab=[word for word in vocab_count.keys()\n","                                if vocab_count[word]>self._min_df]\n","            self._vocab.sort()\n","            return data\n","        \n","        data_train=get_data_and_vocab(path_train)\n","        data_train_embed=self.get_embedding(data_train)\n","        return data_train_embed\n","        \n","    def get_embedding(self,data):\n","        vocab_id=dict([word, self._vocab.index(word)+2] for word in self._vocab) \n","        padding_ID=0\n","        unknown_ID=1\n","        data_embed=[]\n","        for doc in data:\n","            label,doc_id,text=doc.split('<fff>')\n","            words=text.split()[:self._max_doc_length]\n","            doc_embed=[]\n","            for word in words:\n","                if self.isappear(word,self._vocab)!=-1:\n","                    doc_embed.append(str(vocab_id[word]))\n","                else:\n","                    doc_embed.append(str(unknown_ID))\n","            if(len(words)<self._max_doc_length):\n","                for _ in range(self._max_doc_length-len(words)):\n","                    doc_embed.append(str(padding_ID))\n","            data_embed.append(label+'<fff>'+doc_id+'<fff>'+str(len(words))+'<fff>'+' '.join(doc_embed))\n","        \n","        return data_embed\n","        \n","    def fit_tranf(self,path_in):\n","        data=[]\n","        list_folder=os.listdir(path_in)\n","        list_folder.sort()\n","        i_folder=0\n","        for folder in list_folder:\n","            path_folder=path_in+'/'+folder\n","            list_doc=os.listdir(path_folder)\n","            list_doc.sort()\n","            for doc in list_doc:\n","                path_doc=path_folder+'/'+doc\n","                with open(path_doc,'r',errors='ignore') as f:\n","                    text=f.read()\n","                words=re.split('\\W+',text)\n","                content=' '.join(words)\n","                ifo_doc=str(i_folder)+'<fff>'+doc+'<fff>'+content\n","                data.append(ifo_doc)\n","            i_folder+=1\n","        \n","        data_embed=self.get_embedding(data)\n","        return data_embed\n","    \n","    def get_vocab(self):\n","        return self._vocab\n","\n","model=Tranf_encode(500,10)\n","train=model.fit('/content/drive/My Drive/Datasets/20news-bydate-train')\n","test=model.fit_tranf('/content/drive/My Drive/Datasets/20news-bydate-test')\n","vocab=model.get_vocab()\n","\n","with open('/content/drive/My Drive/Datasets/trainencode.txt','w') as f:\n","    f.write('\\n'.join(train))\n","with open('/content/drive/My Drive/Datasets/testencode.txt','w') as f:\n","    f.write('\\n'.join(test))\n","with open('/content/drive/My Drive/Datasets/vocab.txt','w') as f:\n","    f.write('\\n'.join(vocab))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyFD_AaQJuf3","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random \n","\n","class DataReader:\n","    def __init__(self,path,batch_size):\n","        with open(path,'r') as f:\n","            d_line=f.read().splitlines()\n","        self.x=[]\n","        self.y=[]\n","        self.sentence_lengths=[]\n","        self.batch_size=batch_size\n","        for d in d_line:\n","            fea=d.split('<fff>')\n","            label,sentence_len,data=int(fea[0]),int(fea[2]),fea[-1]\n","            self.y.append(label)\n","            self.sentence_lengths.append(sentence_len)\n","            vector=[int(number) for number in data.split()]\n","            self.x.append(np.array(vector))\n","        \n","        self.n_doc=len(d_line)\n","        self.n_batch=int(np.ceil(self.n_doc/batch_size))\n","        rd=random.sample(range(self.n_doc),self.n_doc)\n","        self.x=np.array(self.x)[rd]\n","        self.y=np.array(self.y)[rd]\n","        self.sentence_lengths=np.array(self.sentence_lengths)[rd]\n","        self.batch_id=0\n","        self.epoch=0\n","    \n","    def next_batch(self):\n","        self.batch_id+=1\n","        if(self.batch_id>=self.n_batch-1):\n","            self.batch_id=0\n","            self.epoch+=1\n","            rd=random.sample(range(self.n_doc),self.n_doc)\n","            self.x=self.x[rd]\n","            self.y=self.y[rd]\n","            self.sentence_lengths=self.sentence_lengths[rd]\n","        \n","        start=self.batch_id*self.batch_size\n","        end=start+self.batch_size\n","        return self.x[start:end],self.y[start:end],self.sentence_lengths[start:end]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD65EOG6JugC","colab_type":"code","colab":{}},"source":["import tensorflow.compat.v1 as tf\n","NUM_CLASSES = 20\n","MAX_DOC_LENGTH = 500\n","\n","class RNN:\n","  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n","    self._vocab_size = vocab_size\n","    self._embedding_size = embedding_size\n","    self._lstm_size = lstm_size\n","    self._batch_size = batch_size\n","\n","    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n","    self._labels = tf.placeholder(tf.int32, shape=[batch_size,])\n","    self._sentence_len = tf.placeholder(tf.int32, shape=[batch_size,])\n","\n","  def embedding_layer(self, indices):\n","    pretrain_vecs = []\n","    pretrain_vecs.append(np.zeros(self._embedding_size))\n","    np.random.seed(2020)\n","    for i in range(self._vocab_size+1):\n","      pretrain_vecs.append(np.random.normal(\n","          loc=0.,\n","          scale=1.,\n","          size=self._embedding_size\n","      ))\n","    pretrain_vecs = np.array(pretrain_vecs)\n","    self._embedding_maxtrix = tf.get_variable(\n","        name = 'embedding',\n","        shape = (self._vocab_size+2, self._embedding_size),\n","        initializer = tf.constant_initializer(pretrain_vecs)\n","    )\n","\n","    return tf.nn.embedding_lookup(self._embedding_maxtrix, indices)\n","  \n","  def LSTM_layer(self, embeddings):\n","    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n","    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n","    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n","\n","    lstm_inputs = tf.unstack(tf.transpose(embeddings, perm=[1,0,2]))\n","    lstm_outputs, last_state = tf.nn.static_rnn(\n","        cell = lstm_cell,\n","        inputs = lstm_inputs,\n","        initial_state = initial_state,\n","        sequence_length = self._sentence_len\n","    )\n","    lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm=[1,0,2]))\n","    lstm_outputs = tf.concat(lstm_outputs, axis=0)\n","\n","    mask = tf.sequence_mask(\n","        lengths = self._sentence_len,\n","        maxlen = MAX_DOC_LENGTH,\n","        dtype=tf.float32\n","    )\n","    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n","    mask = tf.expand_dims(mask, -1)\n","\n","    lstm_outputs = mask*lstm_outputs\n","    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n","    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n","    lstm_outputs_aver = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_len, tf.float32), -1)\n","\n","    return lstm_outputs_aver\n","\n","  def build_graph(self):\n","    embeddings = self.embedding_layer(self._data)\n","    lstm_outputs = self.LSTM_layer(embeddings)\n","    weights = tf.get_variable(\n","        name = 'final_layer_weights',\n","        shape = (self._lstm_size, NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed=2020)\n","    )\n","    biases = tf.get_variable(\n","        name = 'final_layer_biases',\n","        shape = (NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed=2020)\n","    )\n","    logits = tf.matmul(lstm_outputs, weights) + biases\n","\n","    label_one_hot = tf.one_hot(\n","        indices = self._labels,\n","        depth = NUM_CLASSES,\n","        dtype = tf.float32\n","    )\n","\n","    loss = tf.nn.softmax_cross_entropy_with_logits(\n","        labels = label_one_hot,\n","        logits = logits\n","    )\n","\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    pred_labels = tf.argmax(probs, axis=1)\n","    pred_labels = tf.squeeze(pred_labels)\n","  \n","    return pred_labels, loss\n","\n","  def trainer(self, loss, lr):\n","    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n","    return train_op\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qbsfnnyb6g2","colab_type":"code","colab":{}},"source":["path_train='/content/drive/My Drive/Datasets/trainencode.txt'\n","path_test='/content/drive/My Drive/Datasets/testencode.txt'\n","path_vocab='/content/drive/My Drive/Datasets/vocab.txt'\n","\n","with open(path_vocab,'r') as f:\n","    vocab_size=len(f.read().splitlines())\n","train_data_reader=DataReader(path_train,batch_size=50)\n","test_data_reader=DataReader(path_test,batch_size=50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHyA3MsVeUeP","colab_type":"code","colab":{}},"source":["def train_and_eval_RNN():\n","    tf.set_random_seed(2020)    \n","    rnn = RNN(\n","        vocab_size=vocab_size,\n","        embedding_size=300,\n","        lstm_size=50,\n","        batch_size=50\n","    )\n","\n","    pred_labels, loss = rnn.build_graph()\n","    op = rnn.trainer(loss=loss, lr=0.01)\n","\n","    with tf.Session() as sess:\n","      step, MAX_STEP = 0, 2000\n","      sess.run(tf.global_variables_initializer())\n","\n","      while step < MAX_STEP:\n","        next_train_batch = train_data_reader.next_batch()\n","        train_data, train_labels, train_sentence_len = next_train_batch\n","        plabel_eval,loss_eval, _ = sess.run(\n","            [pred_labels, loss, op],\n","            feed_dict={\n","                rnn._data: train_data,\n","                rnn._labels: train_labels,\n","                rnn._sentence_len: train_sentence_len\n","            }\n","        )\n","        step += 1\n","        if step % 50 == 0:\n","          print(\"step: \" + str(step) + \" loss: \" + str(loss_eval))\n","        if train_data_reader.batch_id == 0:\n","          num_pre_true=0\n","          while True:\n","            next_test_batch = test_data_reader.next_batch()\n","            test_data, test_labels, test_sentence_len = next_test_batch\n","            test_plabel_eval= sess.run(\n","                pred_labels,\n","                feed_dict={\n","                    rnn._data: test_data,\n","                    rnn._labels: test_labels,\n","                    rnn._sentence_len: test_sentence_len\n","                }\n","            )\n","            matches = np.equal(test_plabel_eval, test_labels)\n","            num_pre_true += np.sum(matches.astype(float))\n","            if test_data_reader.batch_id == 0:\n","                break\n","          print(\"Epoch: \", train_data_reader.epoch)\n","          print(\"Test accuracy: \", num_pre_true / len(test_data_reader.y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gG6w7MMBJugN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"35201c4b-85ea-486f-a160-b20477b86ad1","executionInfo":{"status":"ok","timestamp":1592225490539,"user_tz":-420,"elapsed":910964,"user":{"displayName":"Lộc Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdPIYq3-ZCetn4yF-rejgbAgwO1HAK6wjU3GOL=s64","userId":"15228701076108481136"}}},"source":["tf.reset_default_graph()\n","train_and_eval_RNN()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-17d3dbeeeeef>:36: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-3-17d3dbeeeeef>:45: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-3-17d3dbeeeeef>:88: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","step: 50 loss: 1.3745732\n","step: 100 loss: 0.8788212\n","step: 150 loss: 0.36795798\n","step: 200 loss: 0.4364363\n","Epoch:  1\n","Test accuracy:  0.7448220924057355\n","step: 250 loss: 0.22916569\n","step: 300 loss: 0.2664759\n","step: 350 loss: 0.21291381\n","step: 400 loss: 0.09855297\n","step: 450 loss: 0.20761845\n","Epoch:  2\n","Test accuracy:  0.752655337227828\n","step: 500 loss: 0.0059289606\n","step: 550 loss: 0.0047130124\n","step: 600 loss: 0.018012054\n","step: 650 loss: 0.025948346\n","Epoch:  3\n","Test accuracy:  0.7518587360594795\n","step: 700 loss: 0.0072208177\n","step: 750 loss: 0.008689296\n","step: 800 loss: 0.002155385\n","step: 850 loss: 0.0024043962\n","step: 900 loss: 0.0012124928\n","Epoch:  4\n","Test accuracy:  0.7638077535847052\n","step: 950 loss: 0.0046232995\n","step: 1000 loss: 0.037782855\n","step: 1050 loss: 0.0049599744\n","step: 1100 loss: 0.050126564\n","Epoch:  5\n","Test accuracy:  0.7669941582580988\n","step: 1150 loss: 0.00046617485\n","step: 1200 loss: 0.00039520918\n","step: 1250 loss: 0.000262868\n","step: 1300 loss: 0.00034239373\n","step: 1350 loss: 0.00054130564\n","Epoch:  6\n","Test accuracy:  0.7651354221986192\n","step: 1400 loss: 0.002225977\n","step: 1450 loss: 0.00022006367\n","step: 1500 loss: 0.0002627524\n","step: 1550 loss: 0.00016698403\n","Epoch:  7\n","Test accuracy:  0.766064790228359\n","step: 1600 loss: 0.00019170193\n","step: 1650 loss: 0.00019973278\n","step: 1700 loss: 0.00016394086\n","step: 1750 loss: 6.603219e-05\n","step: 1800 loss: 8.544473e-05\n","Epoch:  8\n","Test accuracy:  0.7655337227827934\n","step: 1850 loss: 8.382389e-05\n","step: 1900 loss: 0.00019898456\n","step: 1950 loss: 0.00016830076\n","step: 2000 loss: 7.862076e-05\n"],"name":"stdout"}]}]}